#!/bin/bash
# SENTINEL Enterprise Upgrade
# Run this in your cloned red2green repo directory
# Usage: bash enterprise_upgrade.sh

set -e
RED='\033[0;31m'; GREEN='\033[0;32m'; YELLOW='\033[1;33m'; CYAN='\033[0;36m'; BOLD='\033[1m'; NC='\033[0m'
ok()   { echo -e "  ${GREEN}âœ“ $1${NC}"; }
step() { echo -e "\n${BOLD}${CYAN}â–¶ $1${NC}"; }

echo -e "${BOLD}${CYAN}"
echo "  ğŸ›¡ï¸  SENTINEL â€” Enterprise LLM Upgrade"
echo "  Adds: Azure OpenAI, AWS Bedrock, Self-hosted Ollama, Groq, Gemini"
echo "  Features: Failover, Audit logging, Data residency mode, Cost tracking"
echo -e "${NC}"

# Must be run inside the repo
if [ ! -f "docker-compose.yml" ] && [ ! -f ".env" ]; then
  echo -e "${RED}âŒ Run this from inside your red2green repo directory${NC}"
  echo "   cd /path/to/red2green && bash enterprise_upgrade.sh"
  exit 1
fi

step "Writing enterprise LLM client"
mkdir -p backend/core backend/agents scripts

cat > backend/core/llm_client.py << 'PYEOF'
"""
SENTINEL Enterprise LLM Client
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Enterprise features:
  âœ… Multi-provider with automatic failover
  âœ… Cost controls + audit logging (every call recorded)
  âœ… Data residency mode (block all external providers)
  âœ… Retry with exponential backoff

Provider Priority (LLM_PROVIDER_ORDER env var):
  Enterprise: azure_openai â†’ aws_bedrock â†’ self_hosted
  Dev/Free:   groq â†’ gemini â†’ anthropic
"""
from __future__ import annotations
import asyncio, hashlib, json, os, time, uuid
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional
import structlog

logger = structlog.get_logger(__name__)

def _env(k, d=""): return os.environ.get(k, d)
def _env_bool(k, d=False): return _env(k, str(d)).lower() in ("1","true","yes")

DATA_RESIDENCY_MODE = _env_bool("SENTINEL_DATA_RESIDENCY", False)
DEFAULT_PROVIDER_ORDER = _env("LLM_PROVIDER_ORDER","groq,gemini,anthropic,self_hosted").split(",")

PROVIDER_INFO = {
    "azure_openai": {
        "name":"Azure OpenAI","enterprise":True,"data_residency":True,
        "compliance":["SOC2","HIPAA","ISO27001","GDPR"],
        "cost":"~$0.003/1K tokens","rate_limit":"Configurable PTU",
        "env_keys":["AZURE_OPENAI_API_KEY","AZURE_OPENAI_ENDPOINT"],
        "setup":"portal.azure.com â†’ Azure OpenAI",
    },
    "aws_bedrock": {
        "name":"AWS Bedrock (Claude)","enterprise":True,"data_residency":True,
        "compliance":["SOC2","HIPAA","ISO27001","FedRAMP"],
        "cost":"~$0.003/1K tokens","rate_limit":"On-demand or provisioned",
        "env_keys":["AWS_ACCESS_KEY_ID","AWS_SECRET_ACCESS_KEY"],
        "setup":"console.aws.amazon.com â†’ Bedrock â†’ Enable Claude",
    },
    "self_hosted": {
        "name":"Self-Hosted Ollama (Qwen2.5-Coder)","enterprise":True,"data_residency":True,
        "compliance":["Any â€” you control infra"],
        "cost":"GPU cost only â€” zero per-token fees","rate_limit":"Your GPU cluster",
        "env_keys":["OLLAMA_BASE_URL"],
        "setup":"ollama.ai â†’ install â†’ ollama pull qwen2.5-coder:32b",
    },
    "groq": {
        "name":"Groq Cloud","enterprise":False,"data_residency":False,
        "compliance":["SOC2 Type II"],
        "cost":"Free: 14,400 req/day","rate_limit":"14,400 req/day",
        "env_keys":["GROQ_API_KEY"],"setup":"console.groq.com",
    },
    "anthropic": {
        "name":"Anthropic API","enterprise":False,"data_residency":False,
        "compliance":["SOC2 Type II"],
        "cost":"$3/1M tokens","rate_limit":"Tier-based",
        "env_keys":["ANTHROPIC_API_KEY"],"setup":"console.anthropic.com",
    },
    "gemini": {
        "name":"Google Gemini","enterprise":False,"data_residency":False,
        "compliance":["SOC2"],
        "cost":"Free: 15 req/min","rate_limit":"15 req/min free",
        "env_keys":["GEMINI_API_KEY"],"setup":"aistudio.google.com/app/apikey",
    },
}

@dataclass
class LLMCallRecord:
    call_id: str; provider: str; model: str; job_id: Optional[str]
    prompt_hash: str; prompt_tokens: int; completion_tokens: int
    latency_ms: int; success: bool; error: Optional[str]; cost_usd: float
    timestamp: datetime = field(default_factory=datetime.utcnow)

_audit_buffer: list[LLMCallRecord] = []

def _audit(r: LLMCallRecord):
    _audit_buffer.append(r)
    logger.info("llm_audit", provider=r.provider,
                tokens=r.prompt_tokens+r.completion_tokens,
                latency_ms=r.latency_ms, success=r.success, cost_usd=r.cost_usd)

def get_audit_log(): return [vars(r) for r in _audit_buffer]

COST_PER_1K = {"azure_openai":0.003,"aws_bedrock":0.003,"self_hosted":0.0,
               "groq":0.0,"anthropic":0.003,"gemini":0.000075}

def _estimate_tokens(t): return len(t)//4
def _estimate_cost(p, inp, out): return (inp+out)/1000 * COST_PER_1K.get(p, 0.003)
def _is_available(p): return all(bool(_env(k)) for k in PROVIDER_INFO.get(p,{}).get("env_keys",[]))
def _is_external(p): return not PROVIDER_INFO.get(p,{}).get("data_residency", False)
def _model_name(p): return {
    "azure_openai":_env("AZURE_OPENAI_DEPLOYMENT","gpt-4o"),
    "aws_bedrock":_env("BEDROCK_MODEL_ID","anthropic.claude-sonnet-4-6-20250514-v1:0"),
    "self_hosted":_env("OLLAMA_MODEL","qwen2.5-coder:32b"),
    "groq":_env("GROQ_MODEL","llama-3.3-70b-versatile"),
    "anthropic":_env("ANTHROPIC_MODEL","claude-sonnet-4-6"),
    "gemini":_env("GEMINI_MODEL","gemini-1.5-flash"),
}.get(p,"unknown")


async def llm_call(prompt:str, temperature:float=0.0, max_tokens:int=4096,
                   job_id:Optional[str]=None, force_provider:Optional[str]=None) -> str:
    """Universal LLM call with enterprise failover + full audit logging."""
    order = [force_provider] if force_provider else DEFAULT_PROVIDER_ORDER
    last_error = None
    for provider in order:
        provider = provider.strip()
        if not _is_available(provider):
            logger.debug("provider_not_configured", provider=provider); continue
        if DATA_RESIDENCY_MODE and _is_external(provider):
            logger.warning("blocked_by_data_residency", provider=provider); continue
        try:
            return await _call_with_audit(provider, prompt, temperature, max_tokens, job_id)
        except Exception as e:
            last_error = e
            logger.warning("provider_failed", provider=provider, error=str(e))
    raise RuntimeError(
        f"All LLM providers failed. Last error: {last_error}\n"
        f"Tried: {order}\n"
        f"â†’ For FREE: Set GROQ_API_KEY at console.groq.com\n"
        f"â†’ For Enterprise: Set AZURE_OPENAI_API_KEY or AWS_ACCESS_KEY_ID"
    )


async def _call_with_audit(provider, prompt, temperature, max_tokens, job_id):
    cid=str(uuid.uuid4())[:8]; pt=_estimate_tokens(prompt)
    ph=hashlib.sha256(prompt.encode()).hexdigest()[:16]; start=time.time()
    try:
        fns={"azure_openai":_azure_openai,"aws_bedrock":_aws_bedrock,
             "self_hosted":_ollama,"groq":_groq,"anthropic":_anthropic,"gemini":_gemini}
        resp = await fns[provider](prompt, temperature, max_tokens)
        ct=_estimate_tokens(resp); ms=int((time.time()-start)*1000)
        _audit(LLMCallRecord(cid,provider,_model_name(provider),job_id,ph,pt,ct,ms,True,None,
                             _estimate_cost(provider,pt,ct)))
        return resp
    except Exception as e:
        ms=int((time.time()-start)*1000)
        _audit(LLMCallRecord(cid,provider,_model_name(provider),job_id,ph,pt,0,ms,False,str(e),0.0))
        raise


async def _azure_openai(prompt, temperature, max_tokens):
    import httpx
    ep=_env("AZURE_OPENAI_ENDPOINT"); key=_env("AZURE_OPENAI_API_KEY")
    dep=_env("AZURE_OPENAI_DEPLOYMENT","gpt-4o"); ver=_env("AZURE_OPENAI_API_VERSION","2024-02-01")
    async with httpx.AsyncClient(timeout=120.0) as c:
        r=await c.post(f"{ep}/openai/deployments/{dep}/chat/completions",
            params={"api-version":ver},
            headers={"api-key":key,"Content-Type":"application/json"},
            json={"messages":[{"role":"user","content":prompt}],
                  "temperature":temperature,"max_tokens":max_tokens})
        r.raise_for_status(); return r.json()["choices"][0]["message"]["content"]

async def _aws_bedrock(prompt, temperature, max_tokens):
    try: import boto3
    except ImportError: raise ImportError("pip install boto3")
    region=_env("AWS_REGION","us-east-1")
    model_id=_env("BEDROCK_MODEL_ID","anthropic.claude-sonnet-4-6-20250514-v1:0")
    client=boto3.client("bedrock-runtime",region_name=region)
    body=json.dumps({"anthropic_version":"bedrock-2023-05-31","max_tokens":max_tokens,
                     "messages":[{"role":"user","content":prompt}],"temperature":temperature})
    loop=asyncio.get_event_loop()
    resp=await loop.run_in_executor(None,lambda:client.invoke_model(modelId=model_id,body=body))
    return json.loads(resp["body"].read())["content"][0]["text"]

async def _ollama(prompt, temperature, max_tokens):
    import httpx
    base=_env("OLLAMA_BASE_URL","http://localhost:11434")
    model=_env("OLLAMA_MODEL","qwen2.5-coder:32b")
    async with httpx.AsyncClient(timeout=300.0) as c:
        r=await c.post(f"{base}/api/chat",json={"model":model,
            "messages":[{"role":"user","content":prompt}],
            "options":{"temperature":temperature,"num_predict":max_tokens},"stream":False})
        r.raise_for_status(); return r.json()["message"]["content"]

async def _groq(prompt, temperature, max_tokens):
    import httpx
    key=_env("GROQ_API_KEY")
    if not key: raise ValueError("GROQ_API_KEY not set")
    model=_env("GROQ_MODEL","llama-3.3-70b-versatile")
    async with httpx.AsyncClient(timeout=60.0) as c:
        r=await c.post("https://api.groq.com/openai/v1/chat/completions",
            headers={"Authorization":f"Bearer {key}","Content-Type":"application/json"},
            json={"model":model,"messages":[{"role":"user","content":prompt}],
                  "temperature":temperature,"max_tokens":max_tokens})
        r.raise_for_status(); return r.json()["choices"][0]["message"]["content"]

async def _anthropic(prompt, temperature, max_tokens):
    import httpx
    key=_env("ANTHROPIC_API_KEY")
    if not key: raise ValueError("ANTHROPIC_API_KEY not set")
    model=_env("ANTHROPIC_MODEL","claude-sonnet-4-6")
    async with httpx.AsyncClient(timeout=120.0) as c:
        r=await c.post("https://api.anthropic.com/v1/messages",
            headers={"x-api-key":key,"anthropic-version":"2023-06-01","Content-Type":"application/json"},
            json={"model":model,"max_tokens":max_tokens,
                  "messages":[{"role":"user","content":prompt}]})
        r.raise_for_status(); return r.json()["content"][0]["text"]

async def _gemini(prompt, temperature, max_tokens):
    import httpx
    key=_env("GEMINI_API_KEY")
    if not key: raise ValueError("GEMINI_API_KEY not set")
    model=_env("GEMINI_MODEL","gemini-1.5-flash")
    async with httpx.AsyncClient(timeout=60.0) as c:
        r=await c.post(
            f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent",
            params={"key":key},
            json={"contents":[{"parts":[{"text":prompt}]}],
                  "generationConfig":{"temperature":temperature,"maxOutputTokens":max_tokens}})
        r.raise_for_status(); return r.json()["candidates"][0]["content"]["parts"][0]["text"]


def print_enterprise_status():
    print("\n" + "â•"*62)
    print("  SENTINEL â€” Enterprise LLM Provider Status")
    print("â•"*62)
    mode="ğŸ”’ ON  (external providers BLOCKED)" if DATA_RESIDENCY_MODE else "âš ï¸  OFF (external providers allowed)"
    print(f"  Data Residency Mode: {mode}")
    print(f"  Provider order: {' â†’ '.join(p.strip() for p in DEFAULT_PROVIDER_ORDER)}\n")
    for p in DEFAULT_PROVIDER_ORDER:
        p=p.strip(); info=PROVIDER_INFO.get(p,{})
        avail=_is_available(p)
        tier="ğŸ¢ Enterprise" if info.get("enterprise") else "â˜ï¸  Cloud   "
        priv="ğŸ”’ On-prem/VPC" if info.get("data_residency") else "ğŸŒ External   "
        status="âœ… Ready    " if avail else "âŒ Not set  "
        print(f"  {status} {tier} {priv} {info.get('name','')}")
        if not avail and info.get("env_keys"):
            print(f"             â†’ Set: {', '.join(info['env_keys'])}")
    cost=sum(r.cost_usd or 0 for r in _audit_buffer)
    print(f"\n  Audit entries: {len(_audit_buffer)}  |  Est. cost: ${cost:.4f}")
    print("â•"*62 + "\n")
PYEOF
ok "Enterprise LLM client written"

step "Writing enterprise .env template"
cat > .env.example << 'EOF'
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  SENTINEL Enterprise Configuration
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Provider order: first available wins, rest are fallbacks
# Enterprise:  azure_openai,aws_bedrock,self_hosted
# Dev/Free:    groq,gemini,anthropic
LLM_PROVIDER_ORDER=groq,gemini,anthropic,self_hosted

# Data Residency: true = BLOCK external providers (HIPAA/PCI/air-gap)
SENTINEL_DATA_RESIDENCY=false

# â”€â”€ ENTERPRISE (data stays in YOUR infra) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Azure OpenAI â€” portal.azure.com â†’ Azure OpenAI â†’ Create
AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE.openai.azure.com
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_DEPLOYMENT=gpt-4o

# AWS Bedrock (Claude) â€” console.aws.amazon.com â†’ Bedrock
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
AWS_REGION=us-east-1
BEDROCK_MODEL_ID=anthropic.claude-sonnet-4-6-20250514-v1:0

# Self-Hosted Ollama â€” ollama.ai â†’ ollama pull qwen2.5-coder:32b
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2.5-coder:32b

# â”€â”€ FREE CLOUD (dev/staging) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Groq â€” FREE 14,400 req/day â€” console.groq.com
GROQ_API_KEY=

# Google Gemini â€” FREE 15 req/min â€” aistudio.google.com/app/apikey
GEMINI_API_KEY=

# Anthropic â€” Paid â€” console.anthropic.com
ANTHROPIC_API_KEY=

# â”€â”€ Infrastructure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GITHUB_TOKEN=
GITHUB_WEBHOOK_SECRET=your_random_secret
POSTGRES_PASSWORD=sentinel_db_secret
DATABASE_URL=postgresql+asyncpg://sentinel:sentinel_db_secret@localhost:5432/sentinel
REDIS_URL=redis://localhost:6379
JWT_SECRET=change_me_in_production

# â”€â”€ Notifications â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SLACK_BOT_TOKEN=
SLACK_CHANNEL_ID=
PAGERDUTY_ROUTING_KEY=
ESCALATION_EMAIL=

# â”€â”€ Observability â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
GRAFANA_PASSWORD=admin
LOG_LEVEL=INFO
EOF
ok "Enterprise .env.example written"

step "Updating test script with enterprise status"
cat > scripts/test_agent.py << 'PYEOF'
"""SENTINEL Agent Test â€” Enterprise Multi-Provider"""
import asyncio, sys, os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'backend'))

# Load .env
env_file = os.path.join(os.path.dirname(__file__), '..', '.env')
if os.path.exists(env_file):
    with open(env_file) as f:
        for line in f:
            line=line.strip()
            if line and not line.startswith('#') and '=' in line:
                k,v=line.split('=',1)
                os.environ.setdefault(k.strip(),v.strip())

from core.llm_client import print_enterprise_status, get_audit_log
from agents.healing_agent import run_healing_job

SAMPLE_ERRORS = {
    "dep_conflict": """
npm ERR! code ERESOLVE
npm ERR! ERESOLVE unable to resolve dependency tree
npm ERR! Found: react@17.0.2
npm ERR! Could not resolve dependency:
npm ERR! peer react@"^18.0.0" from @mui/material@5.15.0
npm ERR! Fix the upstream dependency conflict, or retry with --legacy-peer-deps
""",
    "type_error": """
src/components/UserContext.tsx(47,12): error TS2322:
Type 'string | undefined' is not assignable to type 'string'.
  Type 'undefined' is not assignable to type 'string'.
""",
    "import_error": """
ModuleNotFoundError: No module named 'pydantic_v1'
File "app/models.py", line 3, in <module>
    from pydantic_v1 import BaseModel
""",
}

async def main():
    import sys
    error_type = sys.argv[1] if len(sys.argv) > 1 else "dep_conflict"
    error_log = SAMPLE_ERRORS.get(error_type, SAMPLE_ERRORS["dep_conflict"])

    print_enterprise_status()
    print(f"ğŸ“‹ Simulating: {error_type}\n")

    result = await run_healing_job(
        job_id="test-001", repo_full_name="charan-happy/red2green",
        provider="github", commit_sha="abc1234", branch="main",
        error_log=error_log,
        pipeline_url="https://github.com/charan-happy/red2green/actions/runs/test",
    )

    print(f"{'='*55}")
    print(f"âœ… Status: {result.status.value}\n")
    if result.diagnosis:
        print(f"ğŸ” Diagnosis:")
        print(f"   Type:        {result.diagnosis.failure_type}")
        print(f"   Root Cause:  {result.diagnosis.root_cause}")
        print(f"   Confidence:  {result.diagnosis.confidence:.0%}")
        print(f"   Approach:    {result.diagnosis.suggested_approach}")
    if result.fix_result:
        print(f"\nğŸ”§ Fix:")
        print(f"   Files:       {len(result.fix_result.patches)}")
        print(f"   Confidence:  {result.fix_result.confidence:.0%}")
        print(f"   Explanation: {result.fix_result.explanation}")
        for p in result.fix_result.patches:
            print(f"   ğŸ“„ {p.filename}: {p.explanation}")
    if result.pr_url:
        print(f"\nğŸ”€ PR: {result.pr_url}")
    if result.errors:
        print(f"\nâš ï¸  Errors: {result.errors}")

    # Show audit log
    log = get_audit_log()
    if log:
        print(f"\nğŸ“Š Audit Log ({len(log)} calls):")
        for entry in log:
            cost = f"${entry['cost_usd']:.4f}" if entry.get('cost_usd') else "$0.0000"
            status = "âœ…" if entry['success'] else "âŒ"
            print(f"   {status} {entry['provider']:15} {entry['latency_ms']:5}ms  tokens:{entry['prompt_tokens']+entry['completion_tokens']:5}  cost:{cost}")

    print(f"\n{'='*55}")
    print("Test complete! To test other failure types:")
    print("  python scripts/test_agent.py dep_conflict")
    print("  python scripts/test_agent.py type_error")
    print("  python scripts/test_agent.py import_error")

if __name__ == "__main__":
    asyncio.run(main())
PYEOF
ok "Test script updated"

step "Committing and pushing to GitHub"
git add -A
git commit -m "feat: enterprise multi-provider LLM with failover + audit logging

Providers supported (in priority order):
  ğŸ¢ Azure OpenAI (your Azure tenant â€” SOC2/HIPAA/GDPR)
  ğŸ¢ AWS Bedrock Claude (your AWS VPC â€” FedRAMP/SOC2)
  ğŸ¢ Self-hosted Ollama/Qwen2.5-Coder (on-prem/air-gap)
  â˜ï¸  Groq (free tier â€” 14,400 req/day)
  â˜ï¸  Google Gemini (free â€” 15 req/min)
  â˜ï¸  Anthropic API (paid fallback)

Enterprise features added:
  - Automatic failover (if provider A fails â†’ try B â†’ try C)
  - Data residency mode (SENTINEL_DATA_RESIDENCY=true blocks external)
  - Full audit logging (every LLM call: provider, tokens, cost, latency)
  - Cost estimation per job
  - Provider health status dashboard

For hackathon demo: set GROQ_API_KEY (free at console.groq.com)
For enterprise prod: set AZURE_OPENAI_API_KEY or AWS_ACCESS_KEY_ID"

git push origin HEAD
echo ""
echo -e "${BOLD}${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BOLD}${GREEN}  âœ… Enterprise upgrade pushed to GitHub!${NC}"
echo -e "${BOLD}${GREEN}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${CYAN}  Next: Get FREE Groq key â†’ console.groq.com${NC}"
echo -e "${CYAN}  Then: Add GROQ_API_KEY to .env and run:${NC}"
echo -e "${BOLD}         python scripts/test_agent.py${NC}"
echo ""
echo -e "${YELLOW}  Enterprise deployment options:${NC}"
echo -e "  Azure:  Set AZURE_OPENAI_API_KEY + AZURE_OPENAI_ENDPOINT"
echo -e "  AWS:    Set AWS_ACCESS_KEY_ID + AWS_SECRET_ACCESS_KEY"
echo -e "  Local:  Install Ollama â†’ ollama pull qwen2.5-coder:32b"
echo ""